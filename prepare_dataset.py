"""
Fine-tuning pipeline Ğ´Ğ»Ñ Qwen2.5-VL Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑ…ĞµĞ¼.

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLaMA-Factory Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Hugging Face Trainer.
"""

import json
import os
import random
import base64
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, field, asdict
from datetime import datetime
import re


# â”€â”€â”€ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class FinetuneConfig:
    # ĞœĞ¾Ğ´ĞµĞ»ÑŒ
    model_name:          str  = "Qwen/Qwen2.5-VL-7B-Instruct"
    output_dir:          str  = "models/qwen-schematic-v1"
    
    # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ
    annotated_dir:       str  = "data/annotated"
    processed_dir:       str  = "data/processed"
    train_split:         float = 0.85
    val_split:           float = 0.10
    test_split:          float = 0.05
    
    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
    num_epochs:          int   = 3
    batch_size:          int   = 1          # VL Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
    gradient_accumulation: int = 8
    learning_rate:       float = 2e-5
    max_seq_len:         int   = 4096
    lora_rank:           int   = 16         # LoRA Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
    lora_alpha:          int   = 32
    lora_dropout:        float = 0.05
    
    # Ğ–ĞµĞ»ĞµĞ·Ğ¾
    bf16:                bool  = True
    use_flash_attention: bool  = True
    gradient_checkpointing: bool = True
    
    # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    logging_steps:       int   = 10
    eval_steps:          int   = 100
    save_steps:          int   = 200
    wandb_project:       str   = "qwen-schematic-agent"


# â”€â”€â”€ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """You are an expert electronics engineer and schematic analyzer.
Your task is to identify and locate all electronic components in circuit schematics.

For each component you MUST provide:
- Unique ID (e.g. R1, C2, Q1)
- Component type (resistor/capacitor/inductor/transistor_npn/transistor_pnp/
  mosfet_n/mosfet_p/diode/zener_diode/led/ic/transformer/connector/
  voltage_source/current_source/ground/power)
- Value if visible (e.g. 10kÎ©, 100nF, 2N2222)
- Bounding box as [x1, y1, x2, y2] in pixels
- Confidence score 0.0â€“1.0

Return ONLY a valid JSON object. No markdown, no explanation.
Schema:
{
  "components": [
    {
      "id": "string",
      "type": "string",
      "value": "string or null",
      "bbox": [x1, y1, x2, y2],
      "confidence": float
    }
  ],
  "circuit_type": "string (amplifier/filter/power_supply/oscillator/digital/other)",
  "warnings": ["string"]
}"""

USER_PROMPT = "Analyze this electrical schematic. Identify all components with their bounding boxes and values."


# â”€â”€â”€ ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DatasetBuilder:
    """
    Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Qwen2.5-VL fine-tuning Ğ¸Ğ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼.
    
    ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°:
    - LLaMA-Factory JSON
    - Hugging Face datasets (Arrow)
    - JSONL (universal)
    """
    
    def __init__(self, config: FinetuneConfig):
        self.cfg = config
        os.makedirs(config.processed_dir, exist_ok=True)
    
    def image_to_base64(self, image_path: str) -> str:
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode("utf-8")
    
    def annotation_to_conversation(self, annotation: dict, image_path: str) -> dict:
        """
        ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ñƒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ² conversation-Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ Qwen-VL.
        
        Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ LLaMA-Factory multimodal:
        {
          "messages": [
            {"role": "system",    "content": "..."},
            {"role": "user",      "content": [{"type": "image"}, {"type": "text", "text": "..."}]},
            {"role": "assistant", "content": "...json..."}
          ],
          "images": ["path/to/image.png"]
        }
        """
        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°
        components = annotation.get("components", [])
        circuit_type = annotation.get("circuit_type", "unknown")
        
        answer = json.dumps({
            "components": [
                {
                    "id":         c["id"],
                    "type":       c["type"],
                    "value":      c.get("value"),
                    "bbox":       c.get("bbox", []),
                    "confidence": round(c.get("confidence", 1.0), 2)
                }
                for c in components if c.get("verified", True)
            ],
            "circuit_type": circuit_type,
            "warnings":     []
        }, ensure_ascii=False, separators=(",", ":"))
        
        return {
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": USER_PROMPT}
                    ]
                },
                {"role": "assistant", "content": answer}
            ],
            "images": [image_path]
        }
    
    def build_augmented_variants(self, annotation: dict, image_path: str) -> list:
        """
        Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ñ‹ (data augmentation Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°).
        """
        variants = []
        
        # Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1: Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹
        variants.append(self.annotation_to_conversation(annotation, image_path))
        
        # Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 2: ÑĞ¿Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿
        resistors = [c for c in annotation.get("components", []) if c["type"] == "resistor"]
        if resistors:
            answer = json.dumps({
                "components": resistors,
                "circuit_type": annotation.get("circuit_type", "unknown"),
                "warnings": []
            }, ensure_ascii=False, separators=(",", ":"))
            variants.append({
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {
                        "role": "user",
                        "content": [
                            {"type": "image"},
                            {"type": "text", "text": "List only the RESISTORS in this schematic with their values and locations."}
                        ]
                    },
                    {"role": "assistant", "content": answer}
                ],
                "images": [image_path]
            })
        
        # Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 3: Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ Ñ‚Ğ¸Ğ¿Ğµ ÑÑ…ĞµĞ¼Ñ‹
        variants.append({
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": f"What type of circuit is this? Return JSON with circuit_type and a brief description."}
                    ]
                },
                {
                    "role": "assistant",
                    "content": json.dumps({
                        "circuit_type": annotation.get("circuit_type", "unknown"),
                        "description": f"This is a {annotation.get('circuit_type','unknown')} circuit with {len(annotation.get('components',[]))} components."
                    })
                }
            ],
            "images": [image_path]
        })
        
        return variants
    
    def load_annotations(self) -> list:
        ann_dir = Path(self.cfg.annotated_dir)
        records = []
        
        for json_file in ann_dir.glob("*.json"):
            try:
                with open(json_file) as f:
                    annotation = json.load(f)
                
                # Ğ˜Ñ‰ĞµĞ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
                stem = json_file.stem
                image_path = None
                for ext in [".png", ".jpg", ".jpeg", ".bmp", ".tif"]:
                    candidate = ann_dir / (stem + ext)
                    if candidate.exists():
                        image_path = str(candidate)
                        break
                
                if image_path:
                    records.append((annotation, image_path))
                else:
                    print(f"âš ï¸  No image found for {json_file.name}")
            
            except Exception as e:
                print(f"âš ï¸  Error loading {json_file}: {e}")
        
        print(f"ğŸ“š Loaded {len(records)} annotated schematics")
        return records
    
    def build_and_split(self, augment: bool = True) -> dict:
        """
        Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° train/val/test.
        
        Returns: {"train": [...], "val": [...], "test": [...]}
        """
        records = self.load_annotations()
        
        if not records:
            print("âŒ No annotations found. Add some via the UI first!")
            return {"train": [], "val": [], "test": []}
        
        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹
        all_examples = []
        for annotation, image_path in records:
            if augment:
                examples = self.build_augmented_variants(annotation, image_path)
            else:
                examples = [self.annotation_to_conversation(annotation, image_path)]
            all_examples.extend(examples)
        
        # ĞŸĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµĞ¼
        random.seed(42)
        random.shuffle(all_examples)
        
        n = len(all_examples)
        n_train = int(n * self.cfg.train_split)
        n_val   = int(n * self.cfg.val_split)
        
        splits = {
            "train": all_examples[:n_train],
            "val":   all_examples[n_train:n_train+n_val],
            "test":  all_examples[n_train+n_val:],
        }
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼
        for split_name, data in splits.items():
            # JSONL Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
            jsonl_path = f"{self.cfg.processed_dir}/{split_name}.jsonl"
            with open(jsonl_path, "w", encoding="utf-8") as f:
                for item in data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            
            # LLaMA-Factory JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
            lf_path = f"{self.cfg.processed_dir}/{split_name}_llamafactory.json"
            with open(lf_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… {split_name:5s}: {len(data):4d} examples â†’ {jsonl_path}")
        
        # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
        self._print_stats(splits)
        
        return splits
    
    def _print_stats(self, splits: dict):
        print("\nğŸ“Š Dataset Statistics:")
        print("â”€" * 50)
        total = sum(len(v) for v in splits.values())
        for split, data in splits.items():
            print(f"  {split:8s}: {len(data):4d} ({len(data)/total*100:.1f}%)")
        
        # Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ñ‚Ğ¸Ğ¿Ñ‹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
        type_counts = {}
        for data in splits.values():
            for example in data:
                try:
                    answer = example["messages"][-1]["content"]
                    parsed = json.loads(answer)
                    for comp in parsed.get("components", []):
                        t = comp.get("type", "unknown")
                        type_counts[t] = type_counts.get(t, 0) + 1
                except Exception:
                    pass
        
        if type_counts:
            print("\n  Component distribution:")
            for ctype, count in sorted(type_counts.items(), key=lambda x: -x[1]):
                print(f"    {ctype:20s}: {count}")


# â”€â”€â”€ LLaMA-Factory ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_llamafactory_config(cfg: FinetuneConfig) -> dict:
    """
    Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ YAML ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ´Ğ»Ñ LLaMA-Factory.
    Ğ—Ğ°Ğ¿ÑƒÑĞº: llamafactory-cli train config/lf_train.yaml
    """
    return {
        "### model": None,
        "model_name_or_path": cfg.model_name,
        
        "### method": None,
        "stage":           "sft",
        "do_train":        True,
        "finetuning_type": "lora",
        "lora_rank":       cfg.lora_rank,
        "lora_alpha":      cfg.lora_alpha,
        "lora_dropout":    cfg.lora_dropout,
        "lora_target":     "all",
        
        "### dataset": None,
        "dataset":          "schematic_train",
        "dataset_dir":      cfg.processed_dir,
        "template":         "qwen2_vl",
        "cutoff_len":       cfg.max_seq_len,
        "overwrite_cache":  True,
        
        "### output": None,
        "output_dir":          cfg.output_dir,
        "logging_steps":       cfg.logging_steps,
        "save_steps":          cfg.save_steps,
        "plot_loss":           True,
        "overwrite_output_dir": True,
        
        "### train": None,
        "per_device_train_batch_size": cfg.batch_size,
        "gradient_accumulation_steps": cfg.gradient_accumulation,
        "learning_rate":               cfg.learning_rate,
        "num_train_epochs":            cfg.num_epochs,
        "lr_scheduler_type":           "cosine",
        "warmup_ratio":                0.1,
        "bf16":                        cfg.bf16,
        "flash_attn":                  "fa2" if cfg.use_flash_attention else "disabled",
        
        "### eval": None,
        "val_size":                    cfg.val_split,
        "per_device_eval_batch_size":  cfg.batch_size,
        "eval_strategy":               "steps",
        "eval_steps":                  cfg.eval_steps,
    }


def generate_llamafactory_dataset_info(cfg: FinetuneConfig) -> dict:
    """
    dataset_info.json Ğ´Ğ»Ñ LLaMA-Factory
    """
    return {
        "schematic_train": {
            "file_name":   "train_llamafactory.json",
            "formatting":  "sharegpt",
            "columns": {
                "messages": "messages",
                "images":   "images"
            },
        },
        "schematic_val": {
            "file_name":   "val_llamafactory.json",
            "formatting":  "sharegpt",
            "columns": {
                "messages": "messages",
                "images":   "images"
            },
        }
    }


# â”€â”€â”€ Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRAIN_SCRIPT = '''#!/bin/bash
# â”€â”€â”€ Fine-tuning Qwen2.5-VL Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑ…ĞµĞ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¢Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: GPU 24GB+ (RTX 3090/4090 Ğ¸Ğ»Ğ¸ A100)

set -e

echo "ğŸ”§ Installing dependencies..."
pip install -q llamafactory transformers accelerate peft datasets
pip install -q qwen-vl-utils pillow opencv-python

echo "ğŸ“¦ Preparing dataset..."
python finetuning/prepare_dataset.py

echo "ğŸš€ Starting training..."
llamafactory-cli train config/lf_train.yaml

echo "âœ… Training complete! Model saved to: models/qwen-schematic-v1"

# ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾: ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ LoRA Ğ²ĞµÑĞ¾Ğ²
echo "ğŸ”— Merging LoRA weights..."
llamafactory-cli export config/lf_export.yaml
'''

DOCKER_COMPOSE = '''version: "3.9"
services:
  trainer:
    image: hiyouga/llamafactory:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - WANDB_API_KEY=${WANDB_API_KEY}
    volumes:
      - .:/workspace
      - ~/.cache/huggingface:/root/.cache/huggingface
    working_dir: /workspace
    command: >
      bash -c "pip install -q qwen-vl-utils &&
               llamafactory-cli train config/lf_train.yaml"
    shm_size: "16gb"
    
  ui:
    build: .
    ports:
      - "7860:7860"
    volumes:
      - .:/workspace
    command: python ui/app.py
'''

DOCKERFILE = '''FROM python:3.11-slim

WORKDIR /workspace

RUN apt-get update && apt-get install -y \\
    libgl1-mesa-glx libglib2.0-0 \\
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 7860
CMD ["python", "ui/app.py"]
'''


# â”€â”€â”€ Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYNTHETIC_DATA_SCRIPT = '''"""
Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ…ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°.
Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ schemdraw Ğ´Ğ»Ñ Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ…ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾.

pip install schemdraw
"""
import schemdraw
import schemdraw.elements as elm
import json, random, os
from pathlib import Path


def generate_rc_filter(output_dir: str, idx: int):
    """RC Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚"""
    with schemdraw.Drawing(show=False) as d:
        d.config(fontsize=12)
        
        r_val = random.choice(["1kÎ©", "4.7kÎ©", "10kÎ©", "47kÎ©", "100kÎ©"])
        c_val = random.choice(["10nF", "47nF", "100nF", "470nF", "1ÂµF"])
        
        V  = d.add(elm.SourceV().up().label("Vin"))
        R1 = d.add(elm.Resistor().right().label(r_val))
        C1 = d.add(elm.Capacitor().down().label(c_val))
        d.add(elm.Line().left())
        
        fname = f"{output_dir}/rc_filter_{idx:04d}.png"
        d.save(fname, dpi=150)
        
        annotation = {
            "image": os.path.basename(fname),
            "circuit_type": "filter",
            "components": [
                {"id": "R1", "type": "resistor",  "value": r_val, "bbox": [], "confidence": 1.0, "verified": True},
                {"id": "C1", "type": "capacitor", "value": c_val, "bbox": [], "confidence": 1.0, "verified": True},
                {"id": "V1", "type": "voltage_source", "value": "Vin", "bbox": [], "confidence": 1.0, "verified": True},
            ],
            "synthetic": True
        }
        return annotation, fname


def generate_transistor_amplifier(output_dir: str, idx: int):
    """Ğ£ÑĞ¸Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ¸ÑÑ‚Ğ¾Ñ€Ğµ"""
    with schemdraw.Drawing(show=False) as d:
        d.config(fontsize=12)
        
        rc_val = random.choice(["1kÎ©", "2.2kÎ©", "4.7kÎ©"])
        rb_val = random.choice(["47kÎ©", "100kÎ©", "220kÎ©"])
        
        d.add(elm.BjtNpn(circle=True).anchor("base").label("Q1\\nBC547"))
        d.add(elm.Resistor().up().at("collector").label(rc_val))
        d.add(elm.Dot().label("Vcc", loc="right"))
        d.add(elm.Resistor().left().at("base").label(rb_val))
        d.add(elm.Dot().label("Vin", loc="left"))
        d.add(elm.Ground().at("emitter"))
        
        fname = f"{output_dir}/amp_{idx:04d}.png"
        d.save(fname, dpi=150)
        
        annotation = {
            "image": os.path.basename(fname),
            "circuit_type": "amplifier",
            "components": [
                {"id": "Q1",  "type": "transistor_npn", "value": "BC547", "bbox": [], "confidence": 1.0, "verified": True},
                {"id": "RC1", "type": "resistor",        "value": rc_val,  "bbox": [], "confidence": 1.0, "verified": True},
                {"id": "RB1", "type": "resistor",        "value": rb_val,  "bbox": [], "confidence": 1.0, "verified": True},
            ],
            "synthetic": True
        }
        return annotation, fname


if __name__ == "__main__":
    out = "data/annotated"
    os.makedirs(out, exist_ok=True)
    
    count = 0
    for i in range(50):
        ann, _ = generate_rc_filter(out, i)
        json_path = f"{out}/rc_filter_{i:04d}.json"
        with open(json_path, "w") as f:
            json.dump(ann, f, indent=2)
        count += 1
    
    for i in range(30):
        ann, _ = generate_transistor_amplifier(out, i)
        json_path = f"{out}/amp_{i:04d}.json"
        with open(json_path, "w") as f:
            json.dump(ann, f, indent=2)
        count += 1
    
    print(f"âœ… Generated {count} synthetic schematics")
'''


# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    cfg = FinetuneConfig()
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
    os.makedirs(cfg.processed_dir, exist_ok=True)
    os.makedirs("config", exist_ok=True)
    os.makedirs("models", exist_ok=True)
    
    # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚
    builder = DatasetBuilder(cfg)
    splits  = builder.build_and_split(augment=True)
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ LLaMA-Factory ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸
    lf_cfg = generate_llamafactory_config(cfg)
    
    # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² YAML (Ğ±ĞµĞ· Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ pyyaml)
    yaml_lines = []
    for k, v in lf_cfg.items():
        if v is None:
            yaml_lines.append(f"\n{k}")
        elif isinstance(v, bool):
            yaml_lines.append(f"{k}: {'true' if v else 'false'}")
        else:
            yaml_lines.append(f"{k}: {v}")
    
    with open("config/lf_train.yaml", "w") as f:
        f.write("\n".join(yaml_lines))
    print("âœ… Saved config/lf_train.yaml")
    
    # dataset_info.json
    ds_info = generate_llamafactory_dataset_info(cfg)
    with open(f"{cfg.processed_dir}/dataset_info.json", "w") as f:
        json.dump(ds_info, f, indent=2)
    print(f"âœ… Saved {cfg.processed_dir}/dataset_info.json")
    
    print("\nğŸš€ Next steps:")
    print("  1. Add annotated schematics via:  python ui/app.py")
    print("  2. Run synthetic data generation: python finetuning/generate_synthetic.py")
    print("  3. Prepare dataset:               python finetuning/prepare_dataset.py")
    print("  4. Start training:                llamafactory-cli train config/lf_train.yaml")
    print("     OR with Docker:                docker-compose up trainer")


if __name__ == "__main__":
    main()
